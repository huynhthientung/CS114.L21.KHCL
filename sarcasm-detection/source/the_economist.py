# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iwov8QrveKpZya6XMQJGnQD2Fj7R0VND
"""

import requests
from bs4 import BeautifulSoup
import json
data = {}
data['root'] = []

NUMBER_OF_PAGES = 50 # vì không tra được ngày tháng chính xác nên ước lượng mỗi section chạy 50 trang

count = 0
menu =['asia', 'united-states', 'the-americas', 'china', 'middle-east-and-africa', 'europe', 'britain','international',
       'business', 'finance-and-economics', 'science-and-technology']
for section in menu:
  for i in range(1, NUMBER_OF_PAGES + 1):
    # https://www.economist.com/asia
      url = "https://www.economist.com/" + str(section) + "?page=" + str(i)
      html = requests.get(url)
      domain = "https://www.economist.com"
      soup = BeautifulSoup(html.text, 'html5lib')
      h2 = soup.find_all('a', {'class': 'headline-link'}, )
      for link in h2:
          mylink = BeautifulSoup(str(link), 'html.parser')
          gettinglink = mylink.find('a', href=True)
          headline = mylink.find('span', {'class':"teaser__headline teaser__headline--sc3"})
          # writer.writerow({'Headline': str(gettinglink.find(text=True)), 'Link': str(gettinglink['href'])})
          data['root'].append({
              'is_sarcastic': 0,
              'headline': str(headline.text),
              'article_link': domain + str(gettinglink['href'])})
          count+=1
with open('theEconomist.json', 'w') as f:
    json.dump(data, f, indent=3)
print('Done')
print("Crawled: ", count)